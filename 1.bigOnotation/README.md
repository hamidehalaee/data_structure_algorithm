Big O is a way of describing the efficiency of algorithms without getting too mired in the details. It describes how the time (or the number of operations needed) it takes to run grows as the size of the input grows.

Big O notation helps us answer the question, "How do our functions or algorithms behave/scale when the size of the inputs increases significantly